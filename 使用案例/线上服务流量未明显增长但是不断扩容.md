早上OP反馈说有个线上服务在不停扩容，一天扩容了好多次，但是流量很平稳，不像是因为流量导致的。

看了下监控，发生扩容时的流量确实很稳定。那就只能是业务的某个请求导致的问题了。

![LB QPS](https://github.com/user-attachments/assets/52e65743-6b20-4e63-a823-997638c12260)

发生扩容的时候load确实很高，一般都是一个请求堵住了，然后用户不停点，没做防刷，然后进程一堆积，load自然就高上去了。

![ecs负载](https://github.com/user-attachments/assets/4444768f-2459-4033-875d-594117ffda72)

堵住了的话无非就是那么几个地方，数据库慢查询，或者因为网络抖动导致MC、REDIS慢了，再有就是业务本身的问题。再看眼监控，所有的外部依赖都没有异常，加之发生扩容的只有这一个业务，那网络抖动也可以排除了。原因肯定是业务自己的了。

![RDS](https://github.com/user-attachments/assets/6a8c3215-3499-4070-a81b-0797aadfc156)

既然是业务本身的问题，那首先要定位到请求。利用一下lb日志，查一下扩容发生时有哪些大于3s的请求

```
域名 and request_time> 3 | select url_extract_path(request_uri) as p, count(1) as cnt group by p order by request_time desc
```

发现还不少，其实也很好理解，cpu被抢占之后，所有请求都得不到足够的cpu，慢下来也正常，日志没什么帮助那怎么在不看源码的情况下分析一下根本原因呢。

首先我想确定一下是不是业务代码自身导致的问题，容器级的监控只能看到pod中的某个container在占用cpu。

但是别忘了，业务可以fork其他程序，想到这里，我就建了一个process监控，结果立即发现所有扩容的时候都存在一个ffmpeg进程，而且cpu占用都排在第一名，哈哈一下就定位到了，连源码都不需要看，马上反馈给业务方来处理一下。

![image](https://github.com/user-attachments/assets/7323b070-550d-4bb4-ab03-960c7e30cfb2)

笑死，如果一开始就有这个看板的话就不需要看那么多别的监控图了。


